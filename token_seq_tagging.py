from models import *
import dataset as ds
import treebank as tb
from pos_tagging_utils import *
from torch.optim.adamw import AdamW
from torch.utils.data.dataloader import DataLoader
from pathlib import Path
# import sys

root_dir_path = Path.home() / 'dev/aseker00/modi'
ft_root_dir_path = Path.home() / 'dev/aseker00/fasttext'
# sys.path.insert(0, str(root_dir_path))

partition = tb.load_lattices(root_dir_path, ['dev', 'test', 'train'])
token_vocab, token_dataset = ds.load_token_dataset(root_dir_path, partition)
char_ft_emb, token_ft_emb, form_ft_emb, lemma_ft_emb = ds.load_token_ft_emb(root_dir_path, ft_root_dir_path, token_vocab)

train_dataset = token_dataset['train']
dev_dataset = token_dataset['dev']
test_dataset = token_dataset['test']
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
dev_dataloader = DataLoader(dev_dataset, batch_size=32)
test_dataloader = DataLoader(test_dataset, batch_size=32)

num_tags = len(token_vocab['tags'])
tag2id = {v: i for i, v in enumerate(token_vocab['tags'])}
token_char_emb = TokenCharRNNEmbedding(char_ft_emb, 300, 1, 0.0)
token_emb = TokenEmbedding(token_ft_emb, token_char_emb, 0.7)
token_encoder = TokenRNN(token_emb.embedding_dim, 300, 1, 0.0)
token_classifier = TokenClassifier(token_emb, token_encoder, 0.0, num_tags)
model = TokenMorphSeqClassifier(token_classifier)
device = None


def run_batch(batch, tagger, optimizer):
    batch = tuple(t.to(device) for t in batch)
    batch_token_seq = batch[0]
    batch_token_lengths = batch[1]
    batch_char_seq = batch[2]
    batch_char_lengths = batch[3]
    batch_tag_seq = batch[4][:, :, 6:9]

    ## 1. Token level tagging
    # token sequences
    max_token_seq = batch_token_lengths.max()
    token_seq = batch_token_seq[:, :max_token_seq].contiguous()
    batch_size = token_seq.shape[0]
    seq_lengths = token_seq.shape[1]
    token_seq_idx = torch.arange(seq_lengths, device=device).repeat(batch_size).view(batch_size, -1)
    token_seq_mask = token_seq_idx < batch_token_lengths.unsqueeze(1)

    # token char sequences
    max_char_seq = batch_char_lengths.max()
    token_char_seq = batch_char_seq[:, :max_token_seq, :max_char_seq].contiguous()
    token_char_lengths = batch_char_lengths[:, :max_token_seq].contiguous()

    # gold token tag sequences
    gold_token_tag_seq = batch_tag_seq[:, :max_token_seq].contiguous()

    # token tag scores
    token_input_seq = (token_seq, token_char_seq, token_char_lengths)
    token_tag_scores = tagger(token_input_seq, batch_token_lengths)

    # token tag losses
    token_tag_losses = tagger.token_classifier.loss(token_tag_scores, gold_token_tag_seq, token_seq_mask)

    # decoded token tag sequence
    decoded_token_tag_seq = tagger.token_classifier.decode(token_tag_scores)

    ## 2. Morph level tagging
    # gold tag sequence
    # reshape [bs, seq_len, 3] tensor (last dimension represents pref,host,suff) into [bs, 3*seq_len]
    gold_tag_seq = gold_token_tag_seq.view(batch_size, -1)
    # filter '_' tags and re-pad filtered tag sequence
    gold_token_tag_seq_mask = gold_tag_seq != tag2id['_']
    gold_tag_seq = [tags[mask] for tags, mask in zip(gold_tag_seq, gold_token_tag_seq_mask)]
    gold_tag_seq = torch.nn.utils.rnn.pad_sequence(gold_tag_seq, batch_first=True)
    # keep track of the filtered tags so that we can reconstruct token level tags
    gold_tag_token_mask_idx = [mask.nonzero().squeeze(dim=1) for mask in gold_token_tag_seq_mask]
    gold_tag_token_mask_idx = torch.nn.utils.rnn.pad_sequence(gold_tag_token_mask_idx, batch_first=True)
    # set new pad mask
    gold_tag_seq_mask = gold_tag_seq != 0

    # decoded tag sequence scores
    # stack pref, host, suff score tensors into [bs, seq_len, 3, tags] tensor
    decoded_tag_scores = torch.stack(token_tag_scores, dim=2)
    # reshape [bs, seq_len, 3, tags] into [bs, 3*seq_len, tags]
    decoded_tag_scores = decoded_tag_scores.view(decoded_tag_scores.shape[0], -1, decoded_tag_scores.shape[-1])
    # filter out all tags generated by padded tokens and re-pad
    decoded_token_tag_seq = [seq[mask] for seq, mask in zip(decoded_token_tag_seq, token_seq_mask)]
    decoded_token_tag_seq = torch.nn.utils.rnn.pad_sequence(decoded_token_tag_seq, batch_first=True)
    # reshape [bs, seq_len, 3] into [bs, 3*seq_len]
    decoded_tag_seq = decoded_token_tag_seq.view(batch_size, -1)
    # filter out both '_' tags as well as '<PAD>' and re-pad both decoded tags and scores.
    # Filtering out '<PAD>' is required later when we we generate the tag mask based on the filtered decoded tag seq
    # (I think <PAD> filtering is required because the crf loss mask mustn't have a <PAD> in it's initial position:
    # Traceback (most recent call last):
    #   File "/Users/Amit/dev/aseker00/modi/morph_seq_pos_tagging.py", line 155, in run_epoch
    #     decoded_tag_seq = tagger.decode_crf(decoded_tag_scores, decoded_tag_seq_mask)
    #   File "/Users/Amit/dev/aseker00/modi/models.py", line 253, in decode_crf
    #     decoded_classes = self.crf.decode(emissions=label_scores, mask=mask)
    #   File "/Users/Amit/miniconda3/envs/modi-env/lib/python3.7/site-packages/torchcrf/__init__.py", line 131, in decode
    #     self._validate(emissions, mask=mask)
    #   File "/Users/Amit/miniconda3/envs/modi-env/lib/python3.7/site-packages/torchcrf/__init__.py", line 167, in _validate
    #     raise ValueError('mask of the first timestep must all be on')
    # ValueError: mask of the first timestep must all be on)
    decoded_token_tag_seq_mask = (decoded_tag_seq != tag2id['_']) & (decoded_tag_seq != tag2id['<PAD>'])
    # decoded_token_tag_seq_mask = decoded_tag_seq != tag2id['_']
    decoded_tag_scores = [tags[mask] for tags, mask in zip(decoded_tag_scores, decoded_token_tag_seq_mask)]
    decoded_tag_scores = torch.nn.utils.rnn.pad_sequence(decoded_tag_scores, batch_first=True)
    decoded_tag_seq = [tags[mask] for tags, mask in zip(decoded_tag_seq, decoded_token_tag_seq_mask)]
    decoded_tag_seq = torch.nn.utils.rnn.pad_sequence(decoded_tag_seq, batch_first=True)
    # This is where we generate a 0 pad mask (this is why we filtered <PAD> predictions earlier
    decoded_tag_seq_mask = decoded_tag_seq != 0

    # keep track of the filtered tags so that we can reconstruct token level tags
    decoded_tag_token_mask_idx = [mask.nonzero().squeeze(dim=1) for mask in decoded_token_tag_seq_mask]
    decoded_tag_token_mask_idx = torch.nn.utils.rnn.pad_sequence(decoded_tag_token_mask_idx, batch_first=True)

    # align decoded scores and gold tags/mask before computing loss
    decoded_len = decoded_tag_scores.shape[1]
    gold_len = gold_tag_seq.shape[1]
    if decoded_len < gold_len:
        fill_len = gold_len - decoded_len
        decoded_tag_scores = F.pad(decoded_tag_scores, (0, 0, 0, fill_len))
    elif gold_len < decoded_len:
        fill_len = decoded_len - gold_len
        gold_tag_seq = F.pad(gold_tag_seq, (0, fill_len))
        gold_tag_seq_mask = F.pad(gold_tag_seq_mask, (0, fill_len))
    # compute loss
    tag_seq_loss = tagger.loss_crf(decoded_tag_scores, gold_tag_seq, gold_tag_seq_mask)
    # reset original decoded scores or gold tags/mask dimensions
    if decoded_len != decoded_tag_scores.shape[1]:
        decoded_tag_scores = decoded_tag_scores[:, :decoded_len, :]
    elif gold_len != gold_tag_seq.shape[1]:
        gold_tag_seq = gold_tag_seq[:, :gold_len]
        gold_tag_seq_mask = gold_tag_seq_mask[:, :gold_len]
    # step
    if optimizer:
        pref_loss, host_loss, suff_loss = token_tag_losses
        pref_loss.backward(retain_graph=True)
        host_loss.backward(retain_graph=True)
        suff_loss.backward(retain_graph=True)
        tag_seq_loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    decoded_tag_outputs = (decoded_tag_scores, decoded_tag_seq_mask, decoded_tag_token_mask_idx)
    gold_tag_outputs = (gold_tag_seq, gold_tag_seq_mask, gold_tag_token_mask_idx)
    token_outputs = (token_seq, batch_token_lengths)
    return decoded_tag_outputs, gold_tag_outputs, tag_seq_loss, token_outputs


def run_epoch(epoch, phase, print_every, data, tagger, optimizer=None, max_steps=None):
    print_loss, epoch_loss = 0, 0
    print_samples, epoch_samples = [], []
    for i, batch in enumerate(data):
        step = i + 1
        decoded_tag_outputs, gold_tag_outputs, decoded_tag_loss, token_outputs = run_batch(batch, tagger, optimizer)
        (decoded_tag_scores, decoded_tag_mask, decoded_tag_idx) = decoded_tag_outputs
        (gold_tags, gold_tag_mask, gold_tag_idx) = gold_tag_outputs
        (token_seq, token_seq_lengths) = token_outputs
        with torch.no_grad():
            decoded_tags = tagger.decode_crf(decoded_tag_scores, decoded_tag_mask)
        gold_token_tags = align_token_tags(gold_tags, gold_tag_idx, gold_tag_mask, token_seq_lengths, 3, tag2id)
        decoded_token_tags = align_token_tags(decoded_tags, decoded_tag_idx, decoded_tag_mask, token_seq_lengths, 3, tag2id)
        decoded_token_tags = torch.nn.utils.rnn.pad_sequence(decoded_token_tags, batch_first=True)
        gold_token_tags = torch.nn.utils.rnn.pad_sequence(gold_token_tags, batch_first=True)
        decoded_token_tags_mask = decoded_token_tags != 0
        gold_token_tags_mask = gold_token_tags != 0
        samples = to_samples(decoded_token_tags, gold_token_tags, decoded_token_tags_mask, gold_token_tags_mask, token_seq, token_seq_lengths, token_vocab)
        print_samples.append(samples)
        epoch_samples.append(samples)
        print_loss += decoded_tag_loss
        epoch_loss += decoded_tag_loss
        if step % print_every == 0:
            print(f'{phase} epoch {epoch} step {step} loss: {print_loss / print_every}')
            print_sample(print_samples[-1][0][-1], print_samples[-1][1][-1])
            print_loss = 0
            print_samples = []
        if max_steps and step == max_steps:
            break
    print(f'{phase} epoch {epoch} total loss: {epoch_loss / len(data)}')
    print_scores(epoch_samples, ['_', '<PAD>'])


for lr in [1e-2]:
    adam = AdamW(model.parameters(), lr=lr)
    for epoch in range(15):
        model.train()
        run_epoch(epoch, 'train', 1, train_dataloader, model, adam)
        with torch.no_grad():
            model.eval()
            run_epoch(epoch, 'test', 1, test_dataloader, model)
